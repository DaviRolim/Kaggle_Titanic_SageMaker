{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "portuguese-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.5.10)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.25.10)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (4.42.1)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "Downloading titanic.zip to /home/ec2-user/SageMaker\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 23.6MB/s]\n",
      "Archive:  titanic.zip\n",
      "  inflating: gender_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!kaggle competitions download -c titanic\n",
    "!unzip titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "headed-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "catholic-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "christian-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Importing data and merging\n",
    "####################################\n",
    "\n",
    "# Reading dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Adding a column in each dataset before merging\n",
    "train['Type'] = 'train'\n",
    "test['Type'] = 'test'\n",
    "\n",
    "# Merging train and test\n",
    "data = train.append(test)\n",
    "\n",
    "####################################\n",
    "# Missing values and new features\n",
    "####################################\n",
    "\n",
    "# Title\n",
    "data['Title'] = data['Name']\n",
    "\n",
    "# Cleaning name and extracting Title\n",
    "for name_string in data['Name']:\n",
    "    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "    \n",
    "# Replacing rare titles \n",
    "mapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n",
    "           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n",
    "           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n",
    "           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n",
    "           \n",
    "data.replace({'Title': mapping}, inplace=True)\n",
    "titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']\n",
    "\n",
    "# Replacing missing age by median/title \n",
    "for title in titles:\n",
    "    age_to_impute = data.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] = age_to_impute\n",
    "    \n",
    "# New feature : Family_size\n",
    "data['Family_Size'] = data['Parch'] + data['SibSp'] + 1\n",
    "data.loc[:,'FsizeD'] = 'Alone'\n",
    "data.loc[(data['Family_Size'] > 1),'FsizeD'] = 'Small'\n",
    "data.loc[(data['Family_Size'] > 4),'FsizeD'] = 'Big'\n",
    "\n",
    "# Replacing missing Fare by median/Pclass \n",
    "fa = data[data[\"Pclass\"] == 3]\n",
    "data['Fare'].fillna(fa['Fare'].median(), inplace = True)\n",
    "\n",
    "#  New feature : Child\n",
    "data.loc[:,'Child'] = 1\n",
    "data.loc[(data['Age'] >= 18),'Child'] =0\n",
    "\n",
    "# New feature : Family Survival (https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83)\n",
    "data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "DEFAULT_SURVIVAL_VALUE = 0.5\n",
    "\n",
    "data['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "for grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "                               \n",
    "    if (len(grp_df) != 1):\n",
    "        # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(ind)['Survived'].max()\n",
    "            smin = grp_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1.0):\n",
    "                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin == 0.0):\n",
    "                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                \n",
    "for _, grp_df in data.groupby('Ticket'):\n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin == 0.0):\n",
    "                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                    \n",
    "####################################\n",
    "# Encoding and pre-modeling\n",
    "####################################                  \n",
    "\n",
    "# dropping useless features\n",
    "data = data.drop(columns = ['Age','Cabin','Embarked','Name','Last_Name',\n",
    "                            'Parch', 'SibSp','Ticket', 'Family_Size'])\n",
    "\n",
    "# Encoding features\n",
    "target_col = [\"Survived\"]\n",
    "id_dataset = [\"Type\"]\n",
    "cat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols ]\n",
    "# numerical columns\n",
    "num_cols   = [x for x in data.columns if x not in cat_cols + target_col + id_dataset]\n",
    "# Binary columns with 2 values\n",
    "bin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n",
    "# Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "# Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "# Duplicating columns for multi value columns\n",
    "data = pd.get_dummies(data = data,columns = multi_cols )\n",
    "# Scaling Numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(data[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns = num_cols)\n",
    "# dropping original values merging scaled values for numerical columns\n",
    "df_data_og = data.copy()\n",
    "data = data.drop(columns = num_cols,axis = 1)\n",
    "data = data.merge(scaled,left_index = True,right_index = True,how = \"left\")\n",
    "data = data.drop(columns = ['PassengerId'],axis = 1)\n",
    "\n",
    "# Target = 1st column\n",
    "cols = data.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('Survived')))\n",
    "data = data.reindex(columns= cols)\n",
    "\n",
    "# Cutting train and test\n",
    "train = data[data['Type'] == 1].drop(columns = ['Type'])\n",
    "test = data[data['Type'] == 0].drop(columns = ['Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sublime-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((736, 19), (155, 19))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "df_train = train[msk]\n",
    "df_valid = train[~msk]\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effective-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/xgboost/train/df_train.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/xgboost/test/df_test.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'sagemaker/titanic/xgboost'\n",
    "train_file = 'df_train.csv';\n",
    "df_train.to_csv(train_file, index=False, header=False)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'df_test.csv';\n",
    "df_valid.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indie-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "#         \"max_depth\":\"5\",\n",
    "#         \"eta\":\"0.2\",\n",
    "#         \"gamma\":\"4\",\n",
    "#         \"min_child_weight\":\"6\",\n",
    "#         \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'titanic-xgb-built-in-algo')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, 'latest') #\"1.2-1\")  #06/03/2021\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          #hyperparameters=hyperparameters, #06/03/2021\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.xlarge', \n",
    "                                          sagemaker_session=session, #06/03/2021\n",
    "                                          output_path=output_path)\n",
    "### Start Working 06/03/2021\n",
    "estimator.set_hyperparameters(\n",
    "    eval_metric='auc',\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    rate_drop=0.3,\n",
    "    tweedie_variance_power=1.4\n",
    ")\n",
    "objective_metric_name = 'validation:auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-patch",
   "metadata": {},
   "source": [
    "### Adding hyperparameter tunning\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/tree/master/hyperparameter_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pressed-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working 06/03/2021\n",
    "# Adding Hyperparameter Tuning\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "hyperparameter_ranges = {\n",
    "    'alpha': ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    'lambda': ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "literary-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Working 06/03/2021\n",
    "# Random Search\n",
    "tuner_log = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=10,\n",
    "    strategy='Random'\n",
    ")\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "\n",
    "train_input = TrainingInput(train_data_s3_path, content_type=content_type)\n",
    "validation_input = TrainingInput(test_data_s3_path, content_type=content_type)\n",
    "\n",
    "tuner_log.fit({'train': train_input, 'validation': validation_input}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "included-brain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Working 06/03/2021\n",
    "# Linear Scaling\n",
    "hyperparameter_ranges_linear = {\n",
    "    'eta': ContinuousParameter(0, 1, scaling_type='Linear'),\n",
    "    'min_child_weight': ContinuousParameter(1, 10, scaling_type='Linear'),\n",
    "    'alpha': ContinuousParameter(0.01, 10, scaling_type=\"Linear\"),\n",
    "    'lambda': ContinuousParameter(0.01, 10, scaling_type=\"Linear\")\n",
    "}\n",
    "tuner_linear = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges_linear,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=10,\n",
    "    strategy='Random'\n",
    ")\n",
    "\n",
    "# custom job name to avoid a duplicate name\n",
    "job_name = tuner_log.latest_tuning_job.job_name + 'linear'\n",
    "tuner_linear.fit({'train': train_input, 'validation': validation_input}, include_cls_metadata=False, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-satellite",
   "metadata": {},
   "source": [
    "### Analyze tuning job results - after tuning job is completed\n",
    "Once the tuning jobs have completed, we can compare the distribution of the hyperparameter configurations chosen in the two cases.\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lasting-stress",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found so far Log:\n",
      "{'CreationTime': datetime.datetime(2021, 3, 6, 11, 44, 23, tzinfo=tzlocal()),\n",
      " 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:auc',\n",
      "                                                 'Value': 0.91075199842453},\n",
      " 'ObjectiveStatus': 'Succeeded',\n",
      " 'TrainingEndTime': datetime.datetime(2021, 3, 6, 11, 47, 23, tzinfo=tzlocal()),\n",
      " 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:475414269301:training-job/xgboost-210306-1143-010-db470969',\n",
      " 'TrainingJobName': 'xgboost-210306-1143-010-db470969',\n",
      " 'TrainingJobStatus': 'Completed',\n",
      " 'TrainingStartTime': datetime.datetime(2021, 3, 6, 11, 46, 30, tzinfo=tzlocal()),\n",
      " 'TunedHyperParameters': {'alpha': '1.31666837097785',\n",
      "                          'lambda': '0.011917045099302331'}}\n",
      "Best model found so far Linear:\n",
      "{'CreationTime': datetime.datetime(2021, 3, 6, 11, 55, 56, tzinfo=tzlocal()),\n",
      " 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:auc',\n",
      "                                                 'Value': 0.9100459814071655},\n",
      " 'ObjectiveStatus': 'Succeeded',\n",
      " 'TrainingEndTime': datetime.datetime(2021, 3, 6, 11, 58, 37, tzinfo=tzlocal()),\n",
      " 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:475414269301:training-job/xgboost-210306-1143linear-019-4cf9eaa1',\n",
      " 'TrainingJobName': 'xgboost-210306-1143linear-019-4cf9eaa1',\n",
      " 'TrainingJobStatus': 'Completed',\n",
      " 'TrainingStartTime': datetime.datetime(2021, 3, 6, 11, 57, 46, tzinfo=tzlocal()),\n",
      " 'TunedHyperParameters': {'alpha': '2.7515355247393347',\n",
      "                          'eta': '0.4589197863248471',\n",
      "                          'lambda': '2.974330268590267',\n",
      "                          'min_child_weight': '2.3339412058791464'}}\n"
     ]
    }
   ],
   "source": [
    "sage_client = boto3.Session().client('sagemaker')\n",
    "tuning_job_result_log = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_log.latest_tuning_job.job_name)\n",
    "tuning_job_result_linear = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_linear.latest_tuning_job.job_name)\n",
    "from pprint import pprint\n",
    "if tuning_job_result_log.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far Log:\")\n",
    "    pprint(tuning_job_result_log['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")\n",
    "\n",
    "if tuning_job_result_linear.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far Linear:\")\n",
    "    pprint(tuning_job_result_linear['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-supervision",
   "metadata": {},
   "source": [
    "### Hosting an endpoint using the tuner object and predicting\n",
    "https://aws.amazon.com/blogs/machine-learning/simplify-machine-learning-with-xgboost-and-amazon-sagemaker/\n",
    "\n",
    "https://www.youtube.com/watch?v=yiiLzvAry1o\n",
    "\n",
    "https://www.youtube.com/watch?v=2xc-dddX0LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "finnish-mention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-03-06 11:47:23 Starting - Preparing the instances for training\n",
      "2021-03-06 11:47:23 Downloading - Downloading input data\n",
      "2021-03-06 11:47:23 Training - Training image download completed. Training in progress.\n",
      "2021-03-06 11:47:23 Uploading - Uploading generated training model\n",
      "2021-03-06 11:47:23 Completed - Training job completed\n",
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "xgb_predictor = tuner_log.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    serializer = CSVSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-arrest",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "instructional-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "elegant-information",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.067606  , 0.87085861, 0.21798635, 0.23259716, 0.91707128,\n",
       "       0.1703591 , 0.78770638, 0.09917094, 0.64056659, 0.04564077,\n",
       "       0.11451034, 0.66265136, 0.89045721, 0.07535743, 0.83802766,\n",
       "       0.88281065, 0.07741386, 0.16620497, 0.14529867, 0.63558495,\n",
       "       0.17684659, 0.90192366, 0.89971453, 0.1119828 , 0.98093814,\n",
       "       0.0295784 , 0.71770281, 0.14340948, 0.08338515, 0.02911644,\n",
       "       0.06721346, 0.05723634, 0.94319779, 0.41366693, 0.2404746 ,\n",
       "       0.1809684 , 0.78428042, 0.03184228, 0.11451034, 0.1672986 ,\n",
       "       0.15539147, 0.17184539, 0.06923978, 0.96346319, 0.9741897 ,\n",
       "       0.12303372, 0.14817947, 0.067606  , 0.98671746, 0.97762555,\n",
       "       0.22591685, 0.08030577, 0.98669887, 0.97659749, 0.10959198,\n",
       "       0.04080033, 0.08589345, 0.14360453, 0.10428815, 0.98753244,\n",
       "       0.15264778, 0.09002801, 0.13125384, 0.49471632, 0.52501965,\n",
       "       0.9110229 , 0.39243606, 0.05503313, 0.30592132, 0.9728862 ,\n",
       "       0.39243606, 0.11259524, 0.78770638, 0.11064842, 0.96331501,\n",
       "       0.05403324, 0.06923978, 0.89045721, 0.07435098, 0.3538138 ,\n",
       "       0.55408138, 0.06782264, 0.11526596, 0.11259524, 0.14031762,\n",
       "       0.11171356, 0.71949118, 0.19330029, 0.72231054, 0.95365793,\n",
       "       0.50059867, 0.08082374, 0.9668678 , 0.11451034, 0.07549162,\n",
       "       0.12303372, 0.98614037, 0.18157724, 0.70022261, 0.0951755 ,\n",
       "       0.88937879, 0.07220345, 0.15821886, 0.12029512, 0.97239774,\n",
       "       0.04215373, 0.067606  , 0.10983294, 0.06923978, 0.07933412,\n",
       "       0.10518992, 0.46850666, 0.80831641, 0.24798615, 0.78250504,\n",
       "       0.05180938, 0.067606  , 0.98529714, 0.13105124, 0.91566133,\n",
       "       0.97813362, 0.03689299, 0.96764719, 0.16620497, 0.15821886,\n",
       "       0.12326521, 0.067606  , 0.95245796, 0.0890467 , 0.09249896,\n",
       "       0.06923978, 0.09913699, 0.0743532 , 0.08274284, 0.16620497,\n",
       "       0.07413669, 0.36086422, 0.12817745, 0.19734475, 0.01585369,\n",
       "       0.01868033, 0.9701044 , 0.19590172, 0.09634905, 0.16930631,\n",
       "       0.03262651, 0.06343222, 0.12951094, 0.14440842, 0.1113635 ,\n",
       "       0.97821379, 0.18157724, 0.12303372, 0.43038195, 0.47156534,\n",
       "       0.18157724, 0.7600618 , 0.19330029, 0.12266586, 0.87085861,\n",
       "       0.73495251, 0.97086596, 0.89427787, 0.12029512, 0.11120547,\n",
       "       0.98428416, 0.23205854, 0.01640597, 0.91522658, 0.78770638,\n",
       "       0.12951094, 0.09794475, 0.17483862, 0.30122492, 0.14544025,\n",
       "       0.97519433, 0.97617632, 0.22611608, 0.97371531, 0.90866083,\n",
       "       0.10959198, 0.13969892, 0.95972258, 0.11259524, 0.98836946,\n",
       "       0.06323671, 0.90833598, 0.50355744, 0.02299936, 0.05815353,\n",
       "       0.14067349, 0.19872433, 0.58442724, 0.07365372, 0.95694387,\n",
       "       0.07824562, 0.91630149, 0.19330029, 0.05565498, 0.61619157,\n",
       "       0.90140015, 0.7969169 , 0.09026823, 0.96174508, 0.08895847,\n",
       "       0.19340549, 0.70207727, 0.0890467 , 0.89427787, 0.12951094,\n",
       "       0.12292866, 0.11451034, 0.08898789, 0.91591531, 0.71360993,\n",
       "       0.10673391, 0.83646923, 0.20820463, 0.9668678 , 0.18996365,\n",
       "       0.95207119, 0.16620497, 0.8667177 , 0.06923978, 0.97339523,\n",
       "       0.92396021, 0.18996365, 0.77322155, 0.07396232, 0.07365372,\n",
       "       0.27849519, 0.89427787, 0.04823873, 0.12951094, 0.19041221,\n",
       "       0.067606  , 0.17684659, 0.11140537, 0.91162449, 0.97821379,\n",
       "       0.85222763, 0.98222566, 0.19041221, 0.09249896, 0.569368  ,\n",
       "       0.50053465, 0.89427787, 0.06833817, 0.93005329, 0.7575959 ,\n",
       "       0.96174508, 0.18996365, 0.64196289, 0.11451034, 0.11451034,\n",
       "       0.10068531, 0.13125384, 0.17616473, 0.9518643 , 0.0951755 ,\n",
       "       0.08306857, 0.15965709, 0.98461932, 0.68406039, 0.05565498,\n",
       "       0.18996365, 0.11861761, 0.10983294, 0.64514029, 0.11787498,\n",
       "       0.27839711, 0.05667645, 0.94098836, 0.58167124, 0.067606  ,\n",
       "       0.91663426, 0.05565498, 0.04510844, 0.05824046, 0.0890467 ,\n",
       "       0.10426579, 0.81925905, 0.3683137 , 0.92447126, 0.53355205,\n",
       "       0.05194478, 0.18996365, 0.0822834 , 0.16620497, 0.067606  ,\n",
       "       0.13090989, 0.80780399, 0.16620497, 0.21876402, 0.06923978,\n",
       "       0.09794475, 0.96174508, 0.02600073, 0.43309328, 0.12375467,\n",
       "       0.067606  , 0.0890467 , 0.04453594, 0.16620497, 0.3472831 ,\n",
       "       0.89393604, 0.40150273, 0.96830511, 0.19595636, 0.64580351,\n",
       "       0.04552021, 0.14340948, 0.0951755 , 0.90945476, 0.98082459,\n",
       "       0.86661768, 0.17684659, 0.0553645 , 0.25393665, 0.08185336,\n",
       "       0.067606  , 0.06923978, 0.12183424, 0.2424791 , 0.98753244,\n",
       "       0.07824562, 0.92095321, 0.19999802, 0.09917094, 0.12817745,\n",
       "       0.98423463, 0.2424791 , 0.08791553, 0.97762555, 0.04491518,\n",
       "       0.07282907, 0.10959198, 0.03100494, 0.08895847, 0.80574793,\n",
       "       0.07365372, 0.14340948, 0.03243352, 0.97587389, 0.5538851 ,\n",
       "       0.49568132, 0.15097046, 0.84101486, 0.09978826, 0.9467693 ,\n",
       "       0.97351098, 0.09263791, 0.10215042, 0.09917094, 0.95245796,\n",
       "       0.19340549, 0.98865438, 0.16620497, 0.07932014, 0.33777612,\n",
       "       0.03051516, 0.96127307, 0.83786607, 0.09249896, 0.98512471,\n",
       "       0.09789367, 0.03133189, 0.1091864 , 0.91359788, 0.10959198,\n",
       "       0.04665307, 0.90701842, 0.05503313, 0.0512003 , 0.98836946,\n",
       "       0.98753244, 0.1276374 , 0.06886443, 0.05951178, 0.56811792,\n",
       "       0.05356798, 0.13152583, 0.23387   , 0.64580351, 0.05815353,\n",
       "       0.98669887, 0.11259524, 0.15097046, 0.04915317, 0.04578244,\n",
       "       0.32794318, 0.97469801, 0.97201002, 0.0621765 , 0.1299846 ,\n",
       "       0.91359788, 0.08082374, 0.98283261, 0.18996365, 0.16620497,\n",
       "       0.96750444, 0.05648954, 0.871059  , 0.09199169, 0.13762939,\n",
       "       0.0890467 , 0.05952063, 0.06865585, 0.86661768, 0.91182423,\n",
       "       0.78572857, 0.90866083, 0.80780399, 0.05667645, 0.96892077,\n",
       "       0.12303372, 0.12951094, 0.94021398])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X Test\n",
    "X_test = test.iloc[:, 1:20]\n",
    "predictions = xgb_predictor.predict(X_test.values).decode('utf-8')\n",
    "predictions = np.fromstring(predictions, sep=',')\n",
    "#predictions = predict(X_test.values, rows=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "smaller-artist",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1\n",
       "5          897         0\n",
       "6          898         1\n",
       "7          899         0\n",
       "8          900         1\n",
       "9          901         0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = predictions\n",
    "submission_df['Survived'] = submission_df['Survived'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "submission_df.to_csv('submissions_xgboost.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "handy-owner",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df['Survived'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "persistent-giving",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 2.77k/2.77k [00:00<00:00, 4.79kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f submissions_xgboost.csv -m \"Submission using xgboost log model tunning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "loaded-frequency",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e5dbbfd5-dab8-45e8-8a15-e91d3f8fe286',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e5dbbfd5-dab8-45e8-8a15-e91d3f8fe286',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 06 Mar 2021 12:53:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete hosted endpoint\n",
    "import boto3\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sm_boto3.delete_endpoint(EndpointName=xgb_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-blame",
   "metadata": {},
   "source": [
    "Without using Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "gothic-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the data type and paths to the training and validation datasets\n",
    "# content_type = \"csv\"\n",
    "\n",
    "# train_input = TrainingInput(train_data_s3_path, content_type=content_type)\n",
    "# validation_input = TrainingInput(test_data_s3_path, content_type=content_type)\n",
    "\n",
    "# # execute the XGBoost training job\n",
    "# estimator.fit({'train': train_input, 'validation': validation_input})\n",
    "\n",
    "# xgb_predictor = estimator.deploy(\n",
    "#     initial_instance_count = 1,\n",
    "#     instance_type = 'ml.m4.xlarge',\n",
    "#     serializer = CSVSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-business",
   "metadata": {},
   "source": [
    "### Using a model running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "authorized-congo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 28 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# To predict without deploying the instance\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pregnant-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'sagemaker/titanic/xgboost/titanic-xgb-built-in-algo/output/sagemaker-xgboost-2021-03-05-18-12-37-598/output/'\n",
    "session.download_data(path='./', bucket='sagemaker-us-east-1-475414269301', \n",
    "                                 key_prefix = model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "opponent-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "t = tarfile.open('model.tar.gz', 'r:gz')\n",
    "t.extractall()\n",
    "model = pkl.load(open('xgboost-model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "wrong-stephen",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1\n",
       "5          897         0\n",
       "6          898         1\n",
       "7          899         0\n",
       "8          900         1\n",
       "9          901         0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction with test data\n",
    "pred = model.predict(xgboost.DMatrix(X_test.values))\n",
    "df_test = pd.read_csv('test.csv')\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = pred\n",
    "submission_df['Survived'] = submission_df['Survived'].apply(lambda x: 1 if x > 0.55 else 0)\n",
    "submission_df.to_csv('submissions_xgboost.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "qualified-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 2.77k/2.77k [00:00<00:00, 5.76kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f submissions_xgboost.csv -m \"Submission using xgboost local\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
