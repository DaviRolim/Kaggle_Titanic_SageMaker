{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genetic-conjunction",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.5.10)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (4.42.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.25.10)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (2.8)\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "Downloading titanic.zip to /home/ec2-user/SageMaker\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 22.7MB/s]\n",
      "Archive:  titanic.zip\n",
      "  inflating: gender_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!kaggle competitions download -c titanic\n",
    "!unzip titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "herbal-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liked-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spare-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Importing data and merging\n",
    "####################################\n",
    "\n",
    "# Reading dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Adding a column in each dataset before merging\n",
    "train['Type'] = 'train'\n",
    "test['Type'] = 'test'\n",
    "\n",
    "# Merging train and test\n",
    "data = train.append(test)\n",
    "\n",
    "####################################\n",
    "# Missing values and new features\n",
    "####################################\n",
    "\n",
    "# Title\n",
    "data['Title'] = data['Name']\n",
    "\n",
    "# Cleaning name and extracting Title\n",
    "for name_string in data['Name']:\n",
    "    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "    \n",
    "# Replacing rare titles \n",
    "mapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n",
    "           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n",
    "           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n",
    "           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n",
    "           \n",
    "data.replace({'Title': mapping}, inplace=True)\n",
    "titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']\n",
    "\n",
    "# Replacing missing age by median/title \n",
    "for title in titles:\n",
    "    age_to_impute = data.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] = age_to_impute\n",
    "    \n",
    "# New feature : Family_size\n",
    "data['Family_Size'] = data['Parch'] + data['SibSp'] + 1\n",
    "data.loc[:,'FsizeD'] = 'Alone'\n",
    "data.loc[(data['Family_Size'] > 1),'FsizeD'] = 'Small'\n",
    "data.loc[(data['Family_Size'] > 4),'FsizeD'] = 'Big'\n",
    "\n",
    "# Replacing missing Fare by median/Pclass \n",
    "fa = data[data[\"Pclass\"] == 3]\n",
    "data['Fare'].fillna(fa['Fare'].median(), inplace = True)\n",
    "\n",
    "#  New feature : Child\n",
    "data.loc[:,'Child'] = 1\n",
    "data.loc[(data['Age'] >= 18),'Child'] =0\n",
    "\n",
    "# New feature : Family Survival (https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83)\n",
    "data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "DEFAULT_SURVIVAL_VALUE = 0.5\n",
    "\n",
    "data['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "for grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "                               \n",
    "    if (len(grp_df) != 1):\n",
    "        # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(ind)['Survived'].max()\n",
    "            smin = grp_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1.0):\n",
    "                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin == 0.0):\n",
    "                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                \n",
    "for _, grp_df in data.groupby('Ticket'):\n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin == 0.0):\n",
    "                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                    \n",
    "####################################\n",
    "# Encoding and pre-modeling\n",
    "####################################                  \n",
    "\n",
    "# dropping useless features\n",
    "data = data.drop(columns = ['Age','Cabin','Embarked','Name','Last_Name',\n",
    "                            'Parch', 'SibSp','Ticket', 'Family_Size'])\n",
    "\n",
    "# Encoding features\n",
    "target_col = [\"Survived\"]\n",
    "id_dataset = [\"Type\"]\n",
    "cat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols ]\n",
    "# numerical columns\n",
    "num_cols   = [x for x in data.columns if x not in cat_cols + target_col + id_dataset]\n",
    "# Binary columns with 2 values\n",
    "bin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n",
    "# Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "# Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "# Duplicating columns for multi value columns\n",
    "data = pd.get_dummies(data = data,columns = multi_cols )\n",
    "# Scaling Numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(data[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns = num_cols)\n",
    "# dropping original values merging scaled values for numerical columns\n",
    "df_data_og = data.copy()\n",
    "data = data.drop(columns = num_cols,axis = 1)\n",
    "data = data.merge(scaled,left_index = True,right_index = True,how = \"left\")\n",
    "data = data.drop(columns = ['PassengerId'],axis = 1)\n",
    "\n",
    "# Target = 1st column\n",
    "cols = data.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('Survived')))\n",
    "data = data.reindex(columns= cols)\n",
    "\n",
    "# Cutting train and test\n",
    "train = data[data['Type'] == 1].drop(columns = ['Type'])\n",
    "test = data[data['Type'] == 0].drop(columns = ['Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "simplified-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proved-launch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 19), (201, 19))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "df_train = train[msk]\n",
    "df_valid = train[~msk]\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "precious-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/xgboost/train/df_train.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/xgboost/test/df_test.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'sagemaker/titanic/xgboost'\n",
    "train_file = 'df_train.csv';\n",
    "df_train.to_csv(train_file, index=False, header=False)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'df_test.csv';\n",
    "df_valid.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opponent-tamil",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-05 18:12:37 Starting - Starting the training job...\n",
      "2021-03-05 18:13:01 Starting - Launching requested ML instancesProfilerReport-1614967957: InProgress\n",
      ".........\n",
      "2021-03-05 18:14:22 Starting - Preparing the instances for training......\n",
      "2021-03-05 18:15:29 Downloading - Downloading input data\n",
      "2021-03-05 18:15:29 Training - Downloading the training image...\n",
      "2021-03-05 18:16:04 Uploading - Uploading generated training model\u001b[34m[2021-03-05 18:16:01.123 ip-10-0-251-101.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 690 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 201 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.11884#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.11884#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.11739#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.11739#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.11594#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.11594#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.11304#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.11159#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.11449#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.11304#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.11015#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.11015#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.11015#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.11159#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.11015#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.11015#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.10580#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.10580#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.10580#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.10580#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.10580#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.10435#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.10435#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.10435#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.10145#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.10000#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.10000#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.10000#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09855#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09855#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09710#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09565#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09565#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09710#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09710#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09710#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09565#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09710#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09565#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09420#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09275#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09130#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.08986#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.08986#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.08986#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.08986#011validation-error:0.16915\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.08551#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.08551#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.08551#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.08406#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.08551#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.08406#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.08551#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.08261#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.07971#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.08116#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.07971#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.07971#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.07826#011validation-error:0.17910\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.07826#011validation-error:0.17910\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.07826#011validation-error:0.16418\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.07826#011validation-error:0.15920\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.07826#011validation-error:0.17910\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.07826#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.07681#011validation-error:0.17413\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.07681#011validation-error:0.17910\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.07681#011validation-error:0.17910\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.07681#011validation-error:0.17910\u001b[0m\n",
      "\n",
      "2021-03-05 18:16:24 Completed - Training job completed\n",
      "Training seconds: 58\n",
      "Billable seconds: 58\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "#         \"max_depth\":\"5\",\n",
    "#         \"eta\":\"0.2\",\n",
    "#         \"gamma\":\"4\",\n",
    "#         \"min_child_weight\":\"6\",\n",
    "#         \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'titanic-xgb-built-in-algo')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "\n",
    "train_input = TrainingInput(train_data_s3_path, content_type=content_type)\n",
    "validation_input = TrainingInput(test_data_s3_path, content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "processed-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Test\n",
    "X_test = test.iloc[:, 1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hawaiian-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 28 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# To predict without deploying the instance\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bright-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'sagemaker/titanic/xgboost/titanic-xgb-built-in-algo/output/sagemaker-xgboost-2021-03-05-18-12-37-598/output/'\n",
    "session.download_data(path='./', bucket='sagemaker-us-east-1-475414269301', \n",
    "                                 key_prefix = model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "insured-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "t = tarfile.open('model.tar.gz', 'r:gz')\n",
    "t.extractall()\n",
    "model = pkl.load(open('xgboost-model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "harmful-accent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1\n",
       "5          897         0\n",
       "6          898         1\n",
       "7          899         0\n",
       "8          900         1\n",
       "9          901         0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction with test data\n",
    "pred = model.predict(xgboost.DMatrix(X_test.values))\n",
    "df_test = pd.read_csv('test.csv')\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = pred\n",
    "submission_df['Survived'] = submission_df['Survived'].apply(lambda x: 1 if x > 0.55 else 0)\n",
    "submission_df.to_csv('submissions_xgboost.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "respective-preserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 2.77k/2.77k [00:00<00:00, 5.76kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f submissions_xgboost.csv -m \"Submission using xgboost right valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-intro",
   "metadata": {},
   "source": [
    "### Hosting an endpoint and predicting\n",
    "https://aws.amazon.com/blogs/machine-learning/simplify-machine-learning-with-xgboost-and-amazon-sagemaker/\n",
    "\n",
    "https://www.youtube.com/watch?v=yiiLzvAry1o\n",
    "\n",
    "https://www.youtube.com/watch?v=2xc-dddX0LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "involved-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "outside-converter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    serializer = CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "classified-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "declared-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test.to_numpy()[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "million-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pressed-coordination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0\n",
       "5          897         0\n",
       "6          898         0\n",
       "7          899         0\n",
       "8          900         0\n",
       "9          901         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = predictions\n",
    "submission_df['Survived'] = submission_df['Survived'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "submission_df.to_csv('submissions_xgboost.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "academic-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 2.77k/2.77k [00:00<00:00, 5.49kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f submissions_xgboost.csv -m \"Submission using xgboost right valid hosted endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exterior-tradition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e70beda8-0141-4650-9e67-969166e5426b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e70beda8-0141-4650-9e67-969166e5426b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Fri, 05 Mar 2021 20:13:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete hosted endpoint\n",
    "import boto3\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sm_boto3.delete_endpoint(EndpointName=xgb_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-anthony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
