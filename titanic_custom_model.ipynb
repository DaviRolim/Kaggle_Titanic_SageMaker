{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chubby-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  titanic.zip\n",
      "  inflating: gender_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!unzip titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bigger-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dried-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(train_data, test_data):\n",
    "    # Returns a concatenated df of training and test set\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
    "\n",
    "def divide_df(all_data):\n",
    "    # Returns divided dfs of training and test set\n",
    "    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n",
    "\n",
    "def get_pclass_dist(df):\n",
    "    \n",
    "    # Creating a dictionary for every passenger class count in every deck\n",
    "    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n",
    "    decks = df.columns.levels[0]    \n",
    "    \n",
    "    for deck in decks:\n",
    "        for pclass in range(1, 4):\n",
    "            try:\n",
    "                count = df[deck][pclass][0]\n",
    "                deck_counts[deck][pclass] = count \n",
    "            except KeyError:\n",
    "                deck_counts[deck][pclass] = 0\n",
    "                \n",
    "    df_decks = pd.DataFrame(deck_counts)    \n",
    "    deck_percentages = {}\n",
    "\n",
    "    # Creating a dictionary for every passenger class percentage in every deck\n",
    "    for col in df_decks.columns:\n",
    "        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n",
    "        \n",
    "    return deck_counts, deck_percentages\n",
    "\n",
    "def extract_surname(data):    \n",
    "    \n",
    "    families = []\n",
    "    \n",
    "    for i in range(len(data)):        \n",
    "        name = data.iloc[i]\n",
    "\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0] \n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "            \n",
    "        family = name_no_bracket.split(',')[0]\n",
    "        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n",
    "        \n",
    "        for c in string.punctuation:\n",
    "            family = family.replace(c, '').strip()\n",
    "            \n",
    "        families.append(family)\n",
    "            \n",
    "    return families\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contained-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_all = concat_df(df_train, df_test)\n",
    "\n",
    "df_train.name = 'Training Set'\n",
    "df_test.name = 'Test Set'\n",
    "df_all.name = 'All Set' \n",
    "\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_all_corr[df_all_corr['Feature 1'] == 'Age']\n",
    "\n",
    "df_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "# Filling the missing values in Embarked with S\n",
    "df_all['Embarked'] = df_all['Embarked'].fillna('S')\n",
    "med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n",
    "df_all['Fare'] = df_all['Fare'].fillna(med_fare)\n",
    "\n",
    "# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\n",
    "df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "\n",
    "df_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n",
    "                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n",
    "\n",
    "\n",
    "\n",
    "all_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\n",
    "# Passenger in the T deck is changed to A\n",
    "idx = df_all[df_all['Deck'] == 'T'].index\n",
    "df_all.loc[idx, 'Deck'] = 'A'\n",
    "# Dropping the Cabin feature\n",
    "df_all.drop(['Cabin'], inplace=True, axis=1)\n",
    "\n",
    "df_train, df_test = divide_df(df_all)\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "\n",
    "survived = df_train['Survived'].value_counts()[1]\n",
    "not_survived = df_train['Survived'].value_counts()[0]\n",
    "survived_per = survived / df_train.shape[0] * 100\n",
    "not_survived_per = not_survived / df_train.shape[0] * 100\n",
    "\n",
    "df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\n",
    "df_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n",
    "\n",
    "df_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\n",
    "df_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)\n",
    "\n",
    "# Training set high correlations\n",
    "corr = df_train_corr_nd['Correlation Coefficient'] > 0.1\n",
    "df_train_corr_nd[corr]\n",
    "# Test set high correlations\n",
    "corr = df_test_corr_nd['Correlation Coefficient'] > 0.1\n",
    "df_test_corr_nd[corr]\n",
    "df_all = concat_df(df_train, df_test)\n",
    "df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n",
    "df_all['Age'] = pd.qcut(df_all['Age'], 10)\n",
    "df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n",
    "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "df_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n",
    "df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n",
    "df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "df_all['Is_Married'] = 0\n",
    "df_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n",
    "df_all['Family'] = extract_surname(df_all['Name'])\n",
    "df_train = df_all.loc[:890]\n",
    "df_test = df_all.loc[891:]\n",
    "dfs = [df_train, df_test]\n",
    "# Creating a list of families and tickets that are occuring in both training and test set\n",
    "non_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\n",
    "non_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n",
    "\n",
    "df_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\n",
    "df_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n",
    "\n",
    "family_rates = {}\n",
    "ticket_rates = {}\n",
    "\n",
    "for i in range(len(df_family_survival_rate)):\n",
    "    # Checking a family exists in both training and test set, and has members more than 1\n",
    "    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n",
    "        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n",
    "\n",
    "for i in range(len(df_ticket_survival_rate)):\n",
    "    # Checking a ticket exists in both training and test set, and has members more than 1\n",
    "    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n",
    "        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n",
    "\n",
    "mean_survival_rate = np.mean(df_train['Survived'])\n",
    "\n",
    "train_family_survival_rate = []\n",
    "train_family_survival_rate_NA = []\n",
    "test_family_survival_rate = []\n",
    "test_family_survival_rate_NA = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Family'][i] in family_rates:\n",
    "        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n",
    "        train_family_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        train_family_survival_rate.append(mean_survival_rate)\n",
    "        train_family_survival_rate_NA.append(0)\n",
    "        \n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Family'].iloc[i] in family_rates:\n",
    "        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n",
    "        test_family_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        test_family_survival_rate.append(mean_survival_rate)\n",
    "        test_family_survival_rate_NA.append(0)\n",
    "        \n",
    "df_train['Family_Survival_Rate'] = train_family_survival_rate\n",
    "df_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\n",
    "df_test['Family_Survival_Rate'] = test_family_survival_rate\n",
    "df_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n",
    "\n",
    "train_ticket_survival_rate = []\n",
    "train_ticket_survival_rate_NA = []\n",
    "test_ticket_survival_rate = []\n",
    "test_ticket_survival_rate_NA = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Ticket'][i] in ticket_rates:\n",
    "        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n",
    "        train_ticket_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        train_ticket_survival_rate.append(mean_survival_rate)\n",
    "        train_ticket_survival_rate_NA.append(0)\n",
    "        \n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Ticket'].iloc[i] in ticket_rates:\n",
    "        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n",
    "        test_ticket_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        test_ticket_survival_rate.append(mean_survival_rate)\n",
    "        test_ticket_survival_rate_NA.append(0)\n",
    "        \n",
    "df_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\n",
    "df_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\n",
    "df_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\n",
    "df_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n",
    "for df in [df_train, df_test]:\n",
    "    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n",
    "    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2  \n",
    "    \n",
    "non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n",
    "\n",
    "for df in dfs:\n",
    "    for feature in non_numeric_features:        \n",
    "        df[feature] = LabelEncoder().fit_transform(df[feature])\n",
    "\n",
    "cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\n",
    "encoded_features = []\n",
    "\n",
    "for df in dfs:\n",
    "    for feature in cat_features:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = df[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = df.index\n",
    "        encoded_features.append(encoded_df)\n",
    "\n",
    "df_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\n",
    "df_test = pd.concat([df_test, *encoded_features[6:]], axis=1)\n",
    "\n",
    "df_all = concat_df(df_train, df_test)\n",
    "drop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n",
    "             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n",
    "            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n",
    "\n",
    "df_all.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "domestic-stuff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.drop(drop_cols, axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bound-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'df_train.csv';\n",
    "df_train.to_csv(train_file, index=False, header=True)\n",
    "#train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "#print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'df_test.csv';\n",
    "df_test.to_csv(test_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regular-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composite-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prompt-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/custom/train/df_train.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-475414269301/sagemaker/titanic/custom/test/df_test.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'sagemaker/titanic/custom'\n",
    "train_data_s3_path = sagemaker_session.upload_data(path='df_train.csv', key_prefix=prefix + \"/train\")\n",
    "test_data_s3_path = sagemaker_session.upload_data(path='df_test.csv', key_prefix=prefix + \"/test\")\n",
    "\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "FRAMEWORK_VERSION = '0.23-1'\n",
    "script_path = 'script.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "utility-tampa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-06 16:19:45 Starting - Starting the training job...\n",
      "2021-03-06 16:20:08 Starting - Launching requested ML instancesProfilerReport-1615047585: InProgress\n",
      "......\n",
      "2021-03-06 16:21:09 Starting - Preparing the instances for training...\n",
      "2021-03-06 16:21:44 Downloading - Downloading input data...\n",
      "2021-03-06 16:22:10 Training - Downloading the training image..\u001b[34m2021-03-06 16:22:24,694 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:24,699 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:24,710 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:24,993 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:28,039 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:28,050 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:28,058 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2021-03-06-16-19-45-422\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-475414269301/sagemaker-scikit-learn-2021-03-06-16-19-45-422/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-475414269301/sagemaker-scikit-learn-2021-03-06-16-19-45-422/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2021-03-06-16-19-45-422\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-475414269301/sagemaker-scikit-learn-2021-03-06-16-19-45-422/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python script.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mextracting arguments\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mreading data\u001b[0m\n",
      "\u001b[34mX_train shape: (891, 43)\u001b[0m\n",
      "\u001b[34my_train shape: (891,)\u001b[0m\n",
      "\u001b[34mX_test shape: (418, 43)\u001b[0m\n",
      "\u001b[34mbuilding training and testing datasets\u001b[0m\n",
      "\u001b[34mFold 1\n",
      "\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.7s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    1.8s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    2.5s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34mFold 1 OOB Score: 0.8539325842696629\n",
      "\u001b[0m\n",
      "\u001b[34mFold 2\n",
      "\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\n",
      "2021-03-06 16:22:30 Training - Training image download completed. Training in progress.\u001b[34m[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.6s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    1.7s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    2.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34mFold 2 OOB Score: 0.844319775596073\n",
      "\u001b[0m\n",
      "\u001b[34mFold 3\n",
      "\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.6s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    1.7s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    2.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34mFold 3 OOB Score: 0.85273492286115\n",
      "\u001b[0m\n",
      "\u001b[34mFold 4\n",
      "\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.6s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    1.8s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    2.5s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34mFold 4 OOB Score: 0.8387096774193549\n",
      "\u001b[0m\n",
      "\u001b[34mFold 5\n",
      "\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.6s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    1.7s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    2.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.3s finished\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\u001b[0m\n",
      "\u001b[34mFold 5 OOB Score: 0.8302945301542777\n",
      "\u001b[0m\n",
      "\u001b[34mAverage OOB Score: 0.8439982980601037\u001b[0m\n",
      "\u001b[34mmodel persisted at /opt/ml/model/model.joblib\u001b[0m\n",
      "\u001b[34m2021-03-06 16:22:57,257 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-06 16:23:10 Uploading - Uploading generated training model\n",
      "2021-03-06 16:23:10 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({\n",
    "    'train': train_data_s3_path,\n",
    "    'test': test_data_s3_path\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "personalized-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn.deploy(instance_type='ml.m4.xlarge',\n",
    "                           initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arctic-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-scikit-learn-2021-03-06-16-23-28-886'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name # in order to use with invoke_endpoint in lambda with API gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "engaged-european",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93885142, -1.01419247, -0.60813278, -0.45617155, -0.20533467,\n",
       "        -0.79985861, -0.58655899, -0.5349335 ,  0.95782629, -0.75592895,\n",
       "         0.75592895, -0.13050529, -0.21213203, -0.30229756, -0.17916128,\n",
       "        -0.14834045, -0.13968606, -0.04897021,  0.52752958, -0.56814154,\n",
       "         2.84375747, -1.35067551, -0.06933752, -0.04897021, -0.04897021,\n",
       "        -0.22999288, -0.47896948,  0.86120071, -0.45617155, -0.04897021,\n",
       "        -0.06933752,  0.80757285, -0.15655607, -0.15655607, -0.72879046,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df[:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriental-warrant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (891, 43)\n",
      "y_train shape: (891,)\n",
      "X_test shape: (418, 35)\n"
     ]
    }
   ],
   "source": [
    "X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\n",
    "y_train = df_train['Survived'].values\n",
    "X_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "peripheral-memphis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 43), (418, 43))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForest and some models require that the test data have the same columns as the training data, so lets fix it\n",
    "X_train_df = pd.DataFrame(X_train, columns=df_train.columns.drop(drop_cols))\n",
    "X_test_df = pd.DataFrame(X_test, columns=df_test.columns.drop(drop_cols))\n",
    "missing_cols = list(set(X_train_df.columns) - set(X_test_df.columns))\n",
    "for col in missing_cols:\n",
    "    X_test_df[col] = 0\n",
    "X_train_df.shape, X_test_df.shape\n",
    "# Should be the same number of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aware-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "realistic-relevance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "891          892         0\n",
       "892          893         0\n",
       "893          894         0\n",
       "894          895         0\n",
       "895          896         1\n",
       "896          897         0\n",
       "897          898         0\n",
       "898          899         0\n",
       "899          900         1\n",
       "900          901         0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(predictions)\n",
    "prediction_df[0] = prediction_df[0].astype(float)\n",
    "y_pred = prediction_df[0].astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = y_pred.values\n",
    "submission_df.to_csv('submission_random_forest.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "continent-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|| 2.77k/2.77k [00:00<00:00, 4.80kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f submission_random_forest.csv -m \"Submission using Random Forest Custom Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parental-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_boto3 = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "familiar-result",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c9c69fac-d981-4bca-92c3-8859059870d2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c9c69fac-d981-4bca-92c3-8859059870d2',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 06 Mar 2021 16:43:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-visibility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
